{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Convolutional Neural Network Representation of Image Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models \n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os\n",
    "import ssl\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "from scipy.spatial import distance as dist \n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable #typo \"axesy\" -> \"axes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dis_of_dis_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data for tNSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the pretrained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vschastlivaia/miniforge3/envs/dimentionality-reduction/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/vschastlivaia/miniforge3/envs/dimentionality-reduction/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /Users/vschastlivaia/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:18<00:00, 13.4MB/s] \n",
      "/Users/vschastlivaia/miniforge3/envs/dimentionality-reduction/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/vschastlivaia/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:39<00:00, 14.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# define another class so that the network can output intermediate layer\n",
    "class AlexNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(*list(alexnet.features.children()))\n",
    "        self.avgpool = alexnet.avgpool\n",
    "        self.classifier = nn.Sequential(*list(alexnet.classifier.children()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        for ii, model in enumerate(self.classifier):\n",
    "            x = model(x)\n",
    "            if ii == 1: # extract fc6 response (classifier (1)) \n",
    "                results.append(x)\n",
    "        return x, results         \n",
    "    \n",
    "# define another class so that the network can output intermediate layer\n",
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(*list(vgg16.features.children()))\n",
    "        self.avgpool = vgg16.avgpool\n",
    "        self.classifier = nn.Sequential(*list(vgg16.classifier.children()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        for ii, model in enumerate(self.classifier):\n",
    "            x = model(x)\n",
    "            if ii == 0: # extract fc6 response (classifier (1)) \n",
    "                results.append(x)\n",
    "        return x, results      \n",
    "    \n",
    "    \n",
    "# feedforward CNN model \n",
    "model=AlexNet()\n",
    "# model=VGG16()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             noise_batch\u001b[38;5;241m.\u001b[39mappend(noise_tensor)\n\u001b[1;32m     26\u001b[0m             class_label\u001b[38;5;241m.\u001b[39mappend(root[\u001b[38;5;241m6\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m---> 28\u001b[0m input_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m noise_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(noise_batch)\n\u001b[1;32m     30\u001b[0m total_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([input_batch, noise_batch], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# path to the image folder\n",
    "filenames  = os.listdir('/data/VOC2007/JPEGImages') # on GitHUB the folder is empty\n",
    "input_batch = []\n",
    "noise_batch = []\n",
    "class_label = []\n",
    "n_img = len(filenames)\n",
    "for i in range(n_img):\n",
    "    if filenames[i].endswith('.jpg'):\n",
    "        img_id = filenames[i].strip('.jpg')\n",
    "        # read the annotation files of images \n",
    "        tree = ET.parse('/data/VOC2007/Annotations/{}.xml'.format(img_id))  # on GitHUB the folder is empty\n",
    "        root = tree.getroot()\n",
    "        if len(root.findall('object'))==1:     # if only single object in the image \n",
    "            filename = '/data/VOC2007/JPEGImages/{}.jpg'.format(img_id)  # on GitHUB the folder is empty\n",
    "            input_image = Image.open(filename)\n",
    "            preprocess = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(227),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "            input_tensor = preprocess(input_image)\n",
    "            input_batch.append(input_tensor)\n",
    "            noise_tensor = transforms.ToTensor()(transforms.functional.crop(input_image, 0, 0, 227, 227)) #top left corner\n",
    "            noise_batch.append(noise_tensor)\n",
    "            class_label.append(root[6][0].text)\n",
    "\n",
    "input_batch = torch.stack(input_batch)\n",
    "noise_batch = torch.stack(noise_batch)\n",
    "total_batch = torch.cat([input_batch, noise_batch], dim=0)\n",
    "\n",
    "# label_dict = {np.unique(class_label)[i]:i for i in range(len(np.unique(class_label)))}\n",
    "n_img = len(class_label)\n",
    "label_dict = {'aeroplane': 4,\n",
    " 'bicycle': 2,\n",
    " 'bird': 18,\n",
    " 'boat': 5,\n",
    " 'bottle': 10,\n",
    " 'bus': 1,\n",
    " 'car': 0,\n",
    " 'cat': 13,\n",
    " 'chair': 7,\n",
    " 'cow': 15,\n",
    " 'diningtable': 9,\n",
    " 'dog': 14,\n",
    " 'horse': 16,\n",
    " 'motorbike': 3,\n",
    " 'person': 19,\n",
    " 'pottedplant': 11,\n",
    " 'sheep': 17,\n",
    " 'sofa': 8,\n",
    " 'train': 6,\n",
    " 'tvmonitor': 12}\n",
    "fmt = matplotlib.ticker.FuncFormatter(lambda x, pos: list(label_dict.keys())[list(label_dict.values()).index(int(x))])\n",
    "class_label_color = [label_dict[i] for i in class_label] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image patches only \n",
    "# tsne on the intermediate layer response \n",
    "with torch.no_grad():\n",
    "    output, intermediate_img = model(input_batch)\n",
    "\n",
    "# Visualize the image classification by pretrained AlexNet \n",
    "output = torch.nn.functional.softmax(output, dim=1).detach().numpy() # softmax\n",
    "# _ = [plt.plot(output[j,:]) for j in range(500)] # classification 500 examples    \n",
    "intermediate_img = intermediate_img[0].detach().numpy()\n",
    "\n",
    "# tsne/umap on intermediate layer response \n",
    "distances = dist.squareform(dist.pdist(intermediate_img,'euclidean'))\n",
    "embedding_tsne = TSNE(n_components=2, metric='euclidean', init='pca', perplexity=20).fit_transform(intermediate_img)\n",
    "embedding_umap = umap.UMAP(metric='euclidean', n_neighbors=100).fit_transform(intermediate_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization \n",
    "fig, axa = plt.subplots(1,3, figsize=(15,4))\n",
    "distances_sorted = dist.squareform(dist.pdist(intermediate_img[np.argsort(class_label),:],'euclidean'))\n",
    "q1, q3 = np.percentile(distances_sorted.flatten(),[25,75])\n",
    "iqr = q3-q1\n",
    "lower = q1 - (1.5*iqr)\n",
    "upper = q3 + (1.5*iqr)\n",
    "im1 = axa[0].imshow(distances_sorted, aspect='equal',cmap='hot'); \n",
    "axa[0].set_title('dissimilarity matrix')    \n",
    "im1.set_clim(lower, upper)\n",
    "plt.colorbar(im1,ax=axa[0], ticks=[0,math.floor(np.max(distances_sorted)*100)/100.0],cax=make_axes_locatable(axa[0]).append_axes(\"right\", size=\"5%\", pad=0.05))\n",
    "axa[0].set_xticks([0, distances_sorted.shape[0]]); axa[0].set_yticks([0, distances_sorted.shape[0]])\n",
    "\n",
    "im2 = axa[1].scatter(embedding_tsne[:n_img, 0], embedding_tsne[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im2, ax=axa[1], format=fmt,ticks=np.arange(20));\n",
    "axa[1].set_xticklabels([]); axa[1].set_xticks([])\n",
    "axa[1].set_yticklabels([]); axa[1].set_yticks([])\n",
    "axa[1].set_title('tsne visualization')\n",
    "\n",
    "im3 = axa[2].scatter(embedding_umap[:n_img, 0], embedding_umap[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im3, ax=axa[2], format=fmt,ticks=np.arange(20));\n",
    "axa[2].set_xticklabels([]); axa[2].set_xticks([])\n",
    "axa[2].set_yticklabels([]); axa[2].set_yticks([])\n",
    "axa[2].set_title('umap visualization')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOD ONLY ON CLUSTER POINTS\n",
    "\n",
    "k = 20 # size of neighborhood  \n",
    "ddistances = dis_of_dis_transform(distances, n_neighbors = k)\n",
    "embedding_tsne_dod = TSNE(n_components=2, metric='precomputed', perplexity=20).fit_transform(ddistances)\n",
    "embedding_umap_dod = umap.UMAP(metric='precomputed', n_neighbors=100).fit_transform(ddistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization \n",
    "fig, axa = plt.subplots(2,3, figsize=(15,8))\n",
    "\n",
    "im1 = axa[0][0].imshow(distances, aspect='equal',cmap='hot'); \n",
    "axa[0][0].set_title('dissimilarity matrix')\n",
    "plt.colorbar(im1,ax=axa[0][0],ticks=[0,math.floor(np.max(distances)*100)/100.0],cax=make_axes_locatable(axa[0][0]).append_axes(\"right\", size=\"5%\", pad=0.05))\n",
    "axa[0][0].set_xticks([0, distances.shape[0]]); axa[0][0].set_yticks([0, distances.shape[0]])\n",
    "\n",
    "im2 = axa[1][0].imshow(ddistances, aspect='equal',cmap='hot'); \n",
    "axa[1][0].set_title('dissimilarity matrix with dod')\n",
    "plt.colorbar(im2,ax=axa[1][0],ticks=[0,math.floor(np.max(ddistances)*100)/100.0],cax=make_axes_locatable(axa[1][0]).append_axes(\"right\", size=\"5%\", pad=0.05))\n",
    "axa[1][0].set_xticks([0, ddistances.shape[0]]); axa[1][0].set_yticks([0, ddistances.shape[0]])\n",
    "\n",
    "im3 = axa[0][1].scatter(embedding_tsne[:n_img, 0], embedding_tsne[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im3, ax=axa[0][1], format=fmt,ticks=np.arange(20));\n",
    "axa[0][1].set_title('tsne visualization');axa[0][1].legend(['nat img','noise'])\n",
    "axa[0][1].set_xticklabels([]); axa[0][1].set_xticks([])\n",
    "axa[0][1].set_yticklabels([]); axa[0][1].set_yticks([])\n",
    "\n",
    "im4 = axa[1][1].scatter(embedding_tsne_dod[:n_img, 0], embedding_tsne_dod[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im4, ax=axa[1][1], format=fmt,ticks=np.arange(20));\n",
    "axa[1][1].set_title('tsne visualization with dod');axa[1][1].legend(['nat img','noise'])\n",
    "axa[1][1].set_xticklabels([]); axa[1][1].set_xticks([])\n",
    "axa[1][1].set_yticklabels([]); axa[1][1].set_yticks([])\n",
    "\n",
    "im5 = axa[0][2].scatter(embedding_umap[:n_img, 0], embedding_umap[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im5, ax=axa[0][2], format=fmt,ticks=np.arange(20));\n",
    "axa[0][2].set_title('umap visualization');axa[0][2].legend(['nat img','noise'])\n",
    "axa[0][2].set_xticklabels([]); axa[0][2].set_xticks([])\n",
    "axa[0][2].set_yticklabels([]); axa[0][2].set_yticks([])\n",
    "\n",
    "im6 = axa[1][2].scatter(embedding_umap_dod[:n_img, 0], embedding_umap_dod[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im6, ax=axa[1][2], format=fmt,ticks=np.arange(20));\n",
    "axa[1][2].set_title('umap visualization with dod');axa[1][2].legend(['nat img','noise'])\n",
    "axa[1][2].set_xticklabels([]); axa[1][2].set_xticks([])\n",
    "axa[1][2].set_yticklabels([]); axa[1][2].set_yticks([])\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of noise points, the application of distance-of-distance transformation introduces some distortion to the original manifold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image patches with noise patches \n",
    "# tsne on the intermediate layer response \n",
    "with torch.no_grad():\n",
    "    output, intermediate = model(total_batch)\n",
    "\n",
    "# tsne/umap on intermediate layer response \n",
    "intermediate = intermediate[0].detach().numpy()\n",
    "embedding_tsne = TSNE(n_components=2, metric='euclidean', init='pca',perplexity = 20).fit_transform(intermediate)\n",
    "embedding_umap = umap.UMAP(metric='euclidean', n_neighbors=100).fit_transform(intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne/umap on dod distance matrix \n",
    "distances = dist.squareform(dist.pdist(intermediate,'euclidean'))\n",
    "ddistances = dis_of_dis_transform(distances, 5)\n",
    "embedding_tsne_dod = TSNE(n_components=2, metric='precomputed', perplexity = 20).fit_transform(ddistances)\n",
    "embedding_umap_dod = umap.UMAP(metric='precomputed', n_neighbors=100).fit_transform(ddistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% visualization\n",
    "fig, axa = plt.subplots(2,3, figsize=(15,8))\n",
    "\n",
    "im1 = axa[0][0].imshow(distances, aspect='equal',cmap='hot'); \n",
    "axa[0][0].set_title('dissimilarity matrix')\n",
    "plt.colorbar(im1,ax=axa[0][0],ticks=[0,math.floor(np.max(distances)*100)/100.0],cax=make_axes_locatable(axa[0][0]).append_axes(\"right\", size=\"5%\", pad=0.05))\n",
    "axa[0][0].set_xticks([0, distances.shape[0]]); axa[0][0].set_yticks([0, distances.shape[0]])\n",
    "\n",
    "im2 = axa[1][0].imshow(ddistances, aspect='equal',cmap='hot'); \n",
    "axa[1][0].set_title('dissimilarity matrix with dod')\n",
    "plt.colorbar(im2,ax=axa[1][0],ticks=[0,math.floor(np.max(ddistances)*100)/100.0],cax=make_axes_locatable(axa[1][0]).append_axes(\"right\", size=\"5%\", pad=0.05))\n",
    "axa[1][0].set_xticks([0, ddistances.shape[0]]); axa[1][0].set_yticks([0, ddistances.shape[0]])\n",
    "\n",
    "im3 = axa[0][1].scatter(embedding_tsne[:n_img, 0], embedding_tsne[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im3, ax=axa[0][1], format=fmt,ticks=np.arange(20));\n",
    "axa[0][1].scatter(embedding_tsne[n_img:, 0], embedding_tsne[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[0][1].set_title('tsne visualization');axa[0][1].legend(['nat img','noise'])\n",
    "axa[0][1].set_xticklabels([]); axa[0][1].set_xticks([])\n",
    "axa[0][1].set_yticklabels([]); axa[0][1].set_yticks([])\n",
    "\n",
    "im4 = axa[1][1].scatter(embedding_tsne_dod[:n_img, 0], embedding_tsne_dod[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im4, ax=axa[1][1], format=fmt,ticks=np.arange(20));\n",
    "axa[1][1].scatter(embedding_tsne_dod[n_img:, 0], embedding_tsne_dod[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[1][1].set_title('tsne visualization with dod');axa[1][1].legend(['nat img','noise'])\n",
    "axa[1][1].set_xticklabels([]); axa[1][1].set_xticks([])\n",
    "axa[1][1].set_yticklabels([]); axa[1][1].set_yticks([])\n",
    "\n",
    "im5 = axa[0][2].scatter(embedding_umap[:n_img, 0], embedding_umap[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im5, ax=axa[0][2], format=fmt,ticks=np.arange(20));\n",
    "axa[0][2].scatter(embedding_umap[n_img:, 0], embedding_umap[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[0][2].set_title('umap visualization');axa[0][2].legend(['nat img','noise'])\n",
    "axa[0][2].set_xticklabels([]); axa[0][2].set_xticks([])\n",
    "axa[0][2].set_yticklabels([]); axa[0][2].set_yticks([])\n",
    "\n",
    "im6 = axa[1][2].scatter(embedding_umap_dod[:n_img, 0], embedding_umap_dod[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im6, ax=axa[1][2], format=fmt,ticks=np.arange(20));\n",
    "axa[1][2].scatter(embedding_umap_dod[n_img:, 0], embedding_umap_dod[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[1][2].set_title('umap visualization with dod');axa[1][2].legend(['nat img','noise'])\n",
    "axa[1][2].set_xticklabels([]); axa[1][2].set_xticks([])\n",
    "axa[1][2].set_yticklabels([]); axa[1][2].set_yticks([])\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With distance-of-distance transformation, the noise cloud that was masking the clustering is now separated from the object manifold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% REVIEW: DOD COMPARED WITH PCA PREP  \n",
    "\n",
    "# pca preprocessing \n",
    "pca = PCA(n_components=min(intermediate.shape))\n",
    "intermediate_pca = pca.fit_transform(intermediate)\n",
    "# plt.plot(pca.explained_variance_ratio_)\n",
    "n_pc = 5\n",
    "intermediate_pca = intermediate_pca[:,:5]\n",
    "distances_PCA = dist.squareform(dist.pdist(intermediate_pca[:,:5],'euclidean'))\n",
    "embedding_tsne_PCA = TSNE(n_components=2, metric='euclidean', init='pca', perplexity= 20).fit_transform(intermediate_pca)\n",
    "embedding_umap_PCA = umap.UMAP(n_components=2, metric='euclidean', n_neighbors=100).fit_transform(intermediate_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization \n",
    "fig, axa = plt.subplots(2,3, figsize=(15,8))\n",
    "im1 = axa[0][0].imshow(distances, aspect='equal',cmap='hot'); \n",
    "axa[0][0].set_title('dissimilarity matrix')\n",
    "plt.colorbar(im1,ax=axa[0][0],ticks=[0,math.floor(np.max(distances)*100)/100.0],cax=make_axes_locatable(axa[0][0]).append_axes(\"right\", size=\"5%\", pad=0.05))\n",
    "axa[0][0].set_xticks([0, distances.shape[0]]); axa[0][0].set_yticks([0, distances.shape[0]])\n",
    "\n",
    "im2 = axa[1][0].imshow(distances_PCA, aspect='equal',cmap='hot'); \n",
    "axa[1][0].set_title('dissimilarity matrix with pca')\n",
    "plt.colorbar(im2,ax=axa[1][0],ticks=[0,math.floor(np.max(distances_PCA)*100)/100.0],cax=make_axes_locatable(axa[1][0]).append_axes(\"right\", size=\"5%\", pad=0.05))\n",
    "axa[1][0].set_xticks([0, distances_PCA.shape[0]]); axa[1][0].set_yticks([0, distances_PCA.shape[0]])\n",
    "\n",
    "im3 = axa[0][1].scatter(embedding_tsne[:n_img, 0], embedding_tsne[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im3, ax=axa[0][1], format=fmt,ticks=np.arange(20));\n",
    "axa[0][1].scatter(embedding_tsne[n_img:, 0], embedding_tsne[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[0][1].set_title('tsne visualization');axa[0][1].legend(['nat img','noise'])\n",
    "axa[0][1].set_xticklabels([]); axa[0][1].set_xticks([])\n",
    "axa[0][1].set_yticklabels([]); axa[0][1].set_yticks([])\n",
    "\n",
    "im4 = axa[1][1].scatter(embedding_tsne_PCA[:n_img, 0], embedding_tsne_PCA[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im4, ax=axa[1][1], format=fmt,ticks=np.arange(20));\n",
    "axa[1][1].scatter(embedding_tsne_PCA[n_img:, 0], embedding_tsne_PCA[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[1][1].set_title('tsne visualization with pca');axa[1][1].legend(['nat img','noise'])\n",
    "axa[1][1].set_xticklabels([]); axa[1][1].set_xticks([])\n",
    "axa[1][1].set_yticklabels([]); axa[1][1].set_yticks([])\n",
    "\n",
    "im5 = axa[0][2].scatter(embedding_umap[:n_img, 0], embedding_umap[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im5, ax=axa[0][2], format=fmt,ticks=np.arange(20));\n",
    "axa[0][2].scatter(embedding_umap[n_img:, 0], embedding_umap[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[0][2].set_title('umap visualization');axa[0][2].legend(['nat img','noise'])\n",
    "axa[0][2].set_xticklabels([]); axa[0][2].set_xticks([])\n",
    "axa[0][2].set_yticklabels([]); axa[0][2].set_yticks([])\n",
    "\n",
    "im6 = axa[1][2].scatter(embedding_umap_PCA[:n_img, 0], embedding_umap_PCA[:n_img, 1],c=class_label_color,cmap='jet',marker='o',s = 5) \n",
    "plt.colorbar(im6, ax=axa[1][2], format=fmt,ticks=np.arange(20));\n",
    "axa[1][2].scatter(embedding_umap_PCA[n_img:, 0], embedding_umap_PCA[n_img:, 1],c='grey',marker='o',s = 5) \n",
    "axa[1][2].set_title('umap visualization with pca');axa[1][2].legend(['nat img','noise'])\n",
    "axa[1][2].set_xticklabels([]); axa[1][2].set_xticks([])\n",
    "axa[1][2].set_yticklabels([]); axa[1][2].set_yticks([])\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other commonly used techniques including PCA preprocessing and PCA initialization cannot achieve the separation of noise points as well as distance-of-distance transformation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dimentionality-reduction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
